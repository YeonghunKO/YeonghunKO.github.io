---
title:  "<17> 노마드 코드 python scrapper 디버깅 과정 2(예외처리)"
excerpt: "replace, if/else"

categories:
  - python scrapper
tags:
  - replace
  - 
  - 
last_modified_at: 2020-08-27T09:06:00-05:00

toc: true
toc_label: "목차"
toc_icon: "cog"
toc_sticky: true
---

> 1. 나름대로 개념 정리.  
> 2. 각 개념에 따른 코드 정리.  


# 예외처리 과정  

## 1. 특수문자
특수문자를 입력했을 경우 internal sever error 가 나는 것이다. 명령프롬프트를 확인해보니 remote에서 결과값이 없다는 것을 확인할 수 있었다. 그래서 remote웹사이트에서 c sharp라고 입력하니 결과값이 나왔다. 따라서 `c#` `c++` 를 replace 해야했다.  

```python
def get_job_data(keyword):
    print("remote job scrapping...")
    jobs = []
    # remote website doesn't support # and + character
    keyword = keyword.replace("+","plus").replace("#","sharp") # +를 plus 로 #을 sharp로 대체한다.
    url = f"https://remoteok.io/remote-{keyword}-jobs"
```
검색결과 이상없다는 것을 확인했다.  


## 2. 검색결과가 진짜 없을 경우  
그럼에도 불구하고 한글이나 다른 특수문자를 입력했을때 검색결과가 없다는 것을 확인 할 수 있다. 따라서 if/else를 사용해 결과값이 없을 경우 NONE이 jobs list에 추가되도록 했다.  

```python
def get_job_data(keyword):
    print("remote job scrapping...")
    jobs = []
    # remote website doesn't support # and + character
    keyword = keyword.replace("+","plus").replace("#","sharp")
    url = f"https://remoteok.io/remote-{keyword}-jobs"
    result = requests.get(url, headers = headers)
    soup = BeautifulSoup(result.text, "html.parser")
    element = soup.find("table", {"id":"jobsboard"})
    if element: # element가 결과전체를 담당하는 table인데 존재할 경우에 아래와 같이 처리되고
        job_data = element.findAll("tr", {"class":"job"})
    else: #존재하지 않을 경우 not found가 명령프롬프트에 뜨면서 None으로 입력되도록 했다.
        print("not found")
        job_data = None
    if job_data:
        for ele in job_data:
            data = extract_job(ele)
            jobs.append(data)
            length = len(jobs)
            # print(length, "remote job found")
            if length >= MAX_COUNT:
                break
            else:
                continue
    return jobs
```  
이로써 거의 모든 버그가 해결되었다고 생각한다. 그러나 아직 추가하고 싶은 기능은 많다. 입력된 검색어가 localstorage에 저장되어 사이트에 표시되는 기능, 로딩중일때 로딩중이라고 표시되는 기능 같은거 말이다. 자바스크립트를 써야하는걸로 알고있다. 기능이 추가되면 그에 관련된 글도 포스팅 할 예정이다.  
사실 서비스가 완성되기까지 걸리는 시간중 반이 디버깅하는데 들어간다고 한다. 그리고 완성되고 나서도 끊임없이 업데이트하고 보수해줘야하니 디버깅이 굉장히 중요하다는 것을 다시 배웠다. 역시 고통있어야 단맛도 안다고 모든 것을 해내고 난뒤에 희열은 굉장하다. 이 기쁨을 초심을 잃지 않았으면 좋겠다.  
참고로 아래 링크를 통해 scrapper에 접속할 수 있다.

> [scrapper website](https://scrapperfinal.yeonghunko.repl.co/) 



