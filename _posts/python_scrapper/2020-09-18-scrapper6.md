---
title:  "<6> 노마드 코드 python scrapper - 보너스!-sqlite에 데이터 담기"
excerpt: "sqlite, 끈기"

categories:
  python/python scrapper
tags:
  - 
  - 
last_modified_at: 2020-08-27T09:06:00-05:00

toc: true
toc_label: "목차"
toc_icon: "cog"
toc_sticky: true

header:
  image: /YeonghunKO.github.io/assets/img/scrap.jpg

layout: splash
---

> 1. 나름대로 개념 정리.  
> 2. 각 개념에 따른 코드 정리.  


# 1. sqlite

scrap 한 데이터를 담는다고 했을때 Courera에서 배운 sqlite 가 생각났다. 그래서 SQL을 이용해 뽑아낸 데이터를 저장해보려 했다.

```python
import requests
import sqlite3
import json
import codecs
from bs4 import BeautifulSoup
# sqlite 를 사용하기 위해 필요할지도 모르는 모듈을 import 해온다.

def extract_job(html):    
    title = html.find("h2", {"class": "mb4"}).find("a")["title"]
    company = html.find("h3", {"class":"fc-black-700"}).find("span").string.strip()
    location = html.find("span", {"class": "fc-black-500"}).string.strip()
    job_id = html["data-jobid"]
    link = f"https://stackoverflow.com/jobs?id={job_id}&q=python"
    
    return title,company,location,link
# 기존의 return은 중괄호 안에 보기좋게 저장되어있었다. dict의 tuple 형태로 말이다. 근데 그렇게 저장하면 각각의 원소를 뽑아내기 힘들어진다. 그래서 위에 처럼 간소화 하자!

def extract_so_jobs(last_page): 
    conn = sqlite3.connect("C:\\Bitnami\\wampstack-7.4.6-1\\apache2\\htdocs\\scrapper.sqlite") # Mention the full path to where SQLite is located.
    cur = conn.cursor()  
    cur.executescript('''
    DROP TABLE IF EXISTS scrapper;

    CREATE TABLE scrapper (title TEXT, company TEXT, location TEXT, link TEXT)
    ''') 
    # 두줄을 적으려면 executescript 함수를 사용할 것.
    # SQL 문법과 python 안에서의 사용법은 이미 python 카테고리안에 설명되어있으니 생략하겠다.

    jobs = [] 
    for page in range(1):
        print(f"Scrapping SO Page:{page}")
        result = requests.get(f"{URL}&pg={page + 1}")
        soup = BeautifulSoup(result.text, "html.parser")
        results = soup.find_all("div", {"class":"-job"} )
        for result in results:
            job = extract_job(result)
            jobs.append(job)     

    # 또 다시 for loop을 만들어준다 jobs 리스트를 순회하기 위해서.  
    for data in jobs:
        title = data[0]
        company = data[1]
        location = data[2]
        link = data[3]
        cur.execute('''INSERT INTO scrapper (title, company, location, link)
            VALUES ( ?, ?, ?, ? )''', ( title, company, location, link ) )
        conn.commit()
    cur.close()
    return jobs

# 생각보다 간단했지만 이걸 알아내기까지 많은 시행착오를 겪었다.
```

된다....된다...된다...드디어 진짜 된다ㅠㅠㅠㅠ 진짜 포기하려고 했었는데... 너무 뿌듯하고 자랑스럽고 벅차고 그렇다..   
발상의 전환이 중요한 것 같다. 사실 jobs[] 안에 있는 값을 python coursera 처럼 js 파일로 따로 뽑아서 open 한 다음 for loop 돌릴려고 했는데
js 파일안에 저장된 리스트가 다듬어지지 않아서 계속 에러 나고 심지어 파일을 찾을 수 없다는 에러까지 나서 멘붕왔다. 그래서 파일을 따로 만들지말고
여기 extract_so_job 함수안에서 jobs[] 안에 저장하고 for loop을 돌리자고 생각하니 그뒤로는 쉽게 술술 풀렸다.  
생각의 폭을 넓히고 생각에 갇히지 말자! 편하게 자유롭게!!

![sqlite](https://yeonghunko.github.io/assets/img/scrap/sqlite.png){:class="img-fluid"}


